{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7a750ee-759d-4828-8b57-6dae7aca985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        input_dim = (config[\"num_id_features\"] + config[\"num_dense_features\"]) * config[\"embedding_dim\"]\n",
    "        hidden_dims = config[\"hidden_dims\"]\n",
    "        layers = [nn.Linear(input_dim, hidden_dims[0])]\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EmbeddingInputLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EmbeddingInputLayer, self).__init__()\n",
    "        \n",
    "        num_id_features = config[\"num_id_features\"]\n",
    "        id_spaces = config[\"id_spaces\"]\n",
    "        embedding_dim = config[\"embedding_dim\"]\n",
    "        self.embedding_layers = []\n",
    "        for fea_name, id_space in id_spaces.items():\n",
    "            embedding_layer = nn.Embedding(id_space+5, embedding_dim, padding_idx=None)\n",
    "            self.embedding_layers.append(embedding_layer)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids_expand = torch.unsqueeze(input_tensor, axis=-1)\n",
    "        \n",
    "        embedded_outputs = []\n",
    "        for i, embedding_layer in enumerate(self.embedding_layers):\n",
    "            embedded_output = embedding_layer(input_ids[:, i])\n",
    "            embedded_outputs.append(embedded_output)\n",
    "\n",
    "        return torch.stack(embedded_outputs, dim=1)\n",
    "    \n",
    "\n",
    "class DenseEmbLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DenseEmbLayer, self).__init__()\n",
    "        \n",
    "        embedding_dim = config[\"embedding_dim\"]\n",
    "        num_dense_features = config[\"num_dense_features\"]\n",
    "        self.dense_embedding_layers = []\n",
    "        for i in range(num_dense_features):\n",
    "            linear_layer = nn.Linear(1, embedding_dim)\n",
    "            self.dense_embedding_layers.append(linear_layer)\n",
    "                                    \n",
    "    def forward(self, input_dense):\n",
    "        \n",
    "        embedded_outputs = []\n",
    "        for i, embedding_layer in enumerate(self.dense_embedding_layers):\n",
    "            embedded_output = embedding_layer(torch.unsqueeze(torch.unsqueeze(input_dense[:, i], axis=-1), axis=-1))\n",
    "            embedded_outputs.append(embedded_output)\n",
    "\n",
    "        return torch.concat(embedded_outputs, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db4cc3b6-b292-40d9-ad9f-7a41fd5938ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(DNNModel, self).__init__()\n",
    "\n",
    "        self.emb_layer = EmbeddingInputLayer(config)\n",
    "        self.dense_emb_layer = DenseEmbLayer(config)\n",
    "        \n",
    "        if config[\"backbone_model\"] == \"transformer\":\n",
    "            self.emb_dim_map = Linear(config[\"embedding_dim\"], config[\"transformer_hidden_size\"])\n",
    "            self.backbone_model = nn.Transformer(d_model=config[\"transformer_hidden_size\"])\n",
    "        else:\n",
    "            self.backbone_model = MLP(config)\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, input_dense):\n",
    "        try:\n",
    "            sparse_emb = self.emb_layer(input_ids)\n",
    "        except:\n",
    "            print(\"input_ids\", input_ids) \n",
    "            print(self.emb_layer.weights)\n",
    "            raise ValueError(\"Input ids\")\n",
    "            \n",
    "            \n",
    "        dense_emb = self.dense_emb_layer(input_dense)\n",
    "        \n",
    "        total_emb = torch.concat([sparse_emb, dense_emb], axis=1)\n",
    "        # print(\"total_emb:\", total_emb.shape)\n",
    "        \n",
    "        # If we use simple mlp, flatten\n",
    "        if self.config[\"backbone_model\"] == \"mlp\":\n",
    "            backbone_input_tensor = torch.flatten(total_emb, start_dim = 1)\n",
    "            \n",
    "        elif self.config[\"backbone_model\"]  == \"transformer\":\n",
    "            backbone_input_tensor = self.emb_dim_map(total_emb)\n",
    "        \n",
    "        output_tensor = self.backbone_model(backbone_input_tensor)\n",
    "            \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6f237df-8c4b-4f20-aa12-d7c8aedad7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class PandasTrainDataset(Dataset):\n",
    "    def __init__(self, data, config, is_train=True):\n",
    "        self.data = data\n",
    "        \n",
    "        self.num_id_features = config[\"num_id_features\"]\n",
    "        self.num_dense_features = config[\"num_dense_features\"]\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row = self.data.iloc[idx]\n",
    "        # print(row)\n",
    "        input_ids = torch.tensor([int(x) for x in row.iloc[:self.num_id_features]], dtype=torch.int32)\n",
    "        input_numerical = torch.tensor([float(x) for x in row.iloc[self.num_id_features:self.num_id_features+self.num_dense_features]], dtype=torch.float)\n",
    "        \n",
    "        if self.is_train:\n",
    "            target = torch.tensor(float(row.iloc[-1]), dtype=torch.float)\n",
    "            return (input_ids, input_numerical), target\n",
    "        else:\n",
    "            return (input_ids, input_numerical)\n",
    "\n",
    "\n",
    "\n",
    "# # Custom Dataset class for handling pandas categorical data\n",
    "# class CsvTrainDataset(Dataset):\n",
    "#     def __init__(self, filename, config, is_train=True):\n",
    "#         # self.data = data\n",
    "#         self.filename = filename\n",
    "        \n",
    "#         self.num_id_features = config[\"num_id_features\"]\n",
    "#         self.num_dense_features = config[\"num_dense_features\"]\n",
    "#         self.is_train = is_train\n",
    "        \n",
    "#         self.size = 0\n",
    "        \n",
    "#         with open(self.filename) as f:\n",
    "#             for line in f:\n",
    "#                 self.size += 1\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         with open(self.filename) as f:\n",
    "#             for i in range(idx-1):\n",
    "#                 f.readline()\n",
    "#             row = f.readline().strip().split(',')\n",
    "        \n",
    "        \n",
    "#         # row = self.data.iloc[idx]\n",
    "#         # print(row)\n",
    "#         input_ids = torch.tensor([int(x) for x in row[:self.num_id_features]], dtype=torch.int32)\n",
    "#         input_numerical = torch.tensor([float(x) for x in row[self.num_id_features:self.num_id_features+self.num_dense_features]], dtype=torch.float)\n",
    "        \n",
    "#         if self.is_train:\n",
    "#             target = torch.tensor(float(row[-1]), dtype=torch.float)\n",
    "#             return (input_ids, input_numerical), target\n",
    "#         else:\n",
    "#             return (input_ids, input_numerical)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ac27c-24e8-4b14-a559-26f1d31ad273",
   "metadata": {},
   "source": [
    "## Batch_size should be at least 256 to get a good result, and it needs about 16G~32G memory per worker in the ps-worker parallel mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17210bcb-7827-4c9f-9cd6-4b6adc28fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_features = ['unitDisplayType', 'brandName', 'bundleId',\n",
    "       'appVersion', 'correctModelName', 'countryCode', 'deviceId',\n",
    "       'osAndVersion', 'connectionType', 'c1', 'c2', 'c3', 'c4', 'size', \n",
    "        'mediationProviderVersion', 'bidFloorPrice']\n",
    "model_config = {\n",
    "    \"num_id_features\":  len(id_features),\n",
    "    \"num_dense_features\": 1,\n",
    "    \"id_spaces\": {\n",
    "        'unitDisplayType': 3,\n",
    "         'brandName': 144,\n",
    "         'bundleId': 18,\n",
    "         'appVersion': 101,\n",
    "         'correctModelName': 2568,\n",
    "         'countryCode': 168,\n",
    "         'deviceId': 40176,\n",
    "         'osAndVersion': 96,\n",
    "         'connectionType': 3,\n",
    "         'c1': 50,\n",
    "         'c2': 9,\n",
    "         'c3': 4,\n",
    "         'c4': 9,\n",
    "         'size': 6,\n",
    "         'mediationProviderVersion': 35,\n",
    "         'bidFloorPrice': 4\n",
    "    },\n",
    "    \"embedding_dim\": 8,\n",
    "    \"hidden_dims\": [128, 32],\n",
    "    \"batch_size\": 32,\n",
    "    \"test_batch_size\": 64,\n",
    "    \"backbone_model\": \"mlp\",\n",
    "    \n",
    "#     \"backbone_model\": \"transformer\",\n",
    "#     \"transformer_hidden_size\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9bb78-1e57-4b0f-8083-079e9ea5e355",
   "metadata": {},
   "source": [
    "## The nrows limitation is because my laptop doesn't have enough memeory. \n",
    "## For large dataset, the stream loader should be used. Here I just use an easy way to make it run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfdcb0ad-fc55-47f0-bb45-476d18617afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv(\"train_dataset.csv\", header=None, nrows=1000000)\n",
    "df_test = pd.read_csv(\"test_dataset.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cad1a184-394e-4730-bf7f-86114509a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import IterableDataset\n",
    "\n",
    "# f_train = open(\"train_dataset.csv\")\n",
    "# train_dataset = IterableDataset.from_generator(f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa1202cd-2ffc-4b5b-93d4-a613cd448b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PandasTrainDataset(df_train, model_config, True)\n",
    "train_dataloader = DataLoader(train_dataset, model_config[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1a90d51-f58e-4583-aafd-609042fa5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PandasTrainDataset(df_test, model_config, False)\n",
    "test_dataloader = DataLoader(test_dataset, model_config[\"test_batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26e74845-819a-4dd8-82de-728ce33d0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# for i, ((input_ids, input_numerics), y) in enumerate(train_dataloader):\n",
    "#     print(input_ids.shape)\n",
    "#     print(input_numerics.shape)\n",
    "#     print(y.shape)\n",
    "#     if i == 100:\n",
    "#         break\n",
    "# for input_ids, input_numerics in test_dataloader:\n",
    "#     print(input_ids.shape)\n",
    "#     print(input_numerics.shape)\n",
    "#     # print(y.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d37b36-4384-4e52-b8ba-ac2e47bf154e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## For Output, we can try to test some map function to utilize the fact: SucBid >= SentPrice\n",
    "## for example SentPrice + K * Sigmoid(score) here. K is a maximum price restriction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ec9f3-a680-4560-98be-d3f84b6300d4",
   "metadata": {},
   "source": [
    "## The lr and weight_decay(L2 Normalization) should be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "749c451e-9f60-4423-85d2-e1cdb89273b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNModel(model_config)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "training_loader = train_dataloader\n",
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        (input_ids, input_dense), labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        sig = nn.Sigmoid()\n",
    "        # print(input_dense[:,0].shape, torch.squeeze(model(input_ids, input_dense)).shape, labels.shape)\n",
    "        outputs = input_dense[:,0] + 20 * sig(torch.squeeze(model(input_ids, input_dense))) \n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(torch.squeeze(outputs), labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        # if i % 1000 == 999:\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            # tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3d66f0c-fe51-4654-bd47-c05394515547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 477.61000794219973\n",
      "  batch 2000 loss: 350.8128782539368\n",
      "  batch 3000 loss: 334.9473261489868\n",
      "  batch 4000 loss: 228.19869411087035\n",
      "  batch 5000 loss: 415.52515966796875\n",
      "  batch 6000 loss: 314.66726699066163\n",
      "  batch 7000 loss: 458.8694084892273\n",
      "  batch 8000 loss: 542.8520246429443\n",
      "  batch 9000 loss: 491.32740203475953\n",
      "  batch 10000 loss: 327.92970405960085\n",
      "  batch 11000 loss: 267.48425094985964\n",
      "  batch 12000 loss: 285.04578519058225\n",
      "  batch 13000 loss: 341.48575658798217\n",
      "  batch 14000 loss: 429.9701660423279\n",
      "  batch 15000 loss: 422.77308324813845\n",
      "  batch 16000 loss: 262.1191298904419\n",
      "  batch 17000 loss: 263.84964530563354\n",
      "  batch 18000 loss: 290.9429146537781\n",
      "  batch 19000 loss: 313.31634185409547\n",
      "  batch 20000 loss: 312.93155073165894\n",
      "  batch 21000 loss: 334.437187297821\n",
      "  batch 22000 loss: 490.98245753479006\n",
      "  batch 23000 loss: 566.5439945259094\n",
      "  batch 24000 loss: 473.8431470413208\n",
      "  batch 25000 loss: 298.2583898925781\n",
      "  batch 26000 loss: 440.8435456466675\n",
      "  batch 27000 loss: 381.86826808166506\n",
      "  batch 28000 loss: 306.6315210762024\n",
      "  batch 29000 loss: 282.61425187301637\n",
      "  batch 30000 loss: 317.35470530319213\n",
      "  batch 31000 loss: 528.9706291046142\n",
      "  batch 32000 loss: 493.3394151535034\n",
      "  batch 33000 loss: 330.9032545967102\n",
      "  batch 34000 loss: 262.49860860824583\n",
      "  batch 35000 loss: 286.10596172714236\n",
      "  batch 36000 loss: 330.14315601348875\n",
      "  batch 37000 loss: 353.14781604766847\n",
      "  batch 38000 loss: 327.7201942062378\n",
      "  batch 39000 loss: 374.45583725738527\n",
      "  batch 40000 loss: 290.75845000457764\n",
      "  batch 41000 loss: 383.578382019043\n",
      "  batch 42000 loss: 297.7098835945129\n",
      "  batch 43000 loss: 285.23431800842286\n",
      "  batch 44000 loss: 387.6884707984924\n",
      "  batch 45000 loss: 285.5949113693237\n",
      "  batch 46000 loss: 426.45165940856936\n",
      "  batch 47000 loss: 252.40414377975463\n",
      "  batch 48000 loss: 482.4924156036377\n",
      "  batch 49000 loss: 340.60509690856935\n",
      "  batch 50000 loss: 278.16205104446414\n",
      "  batch 51000 loss: 330.4836669006348\n",
      "  batch 52000 loss: 412.37803088760376\n",
      "  batch 53000 loss: 281.01850724029543\n",
      "  batch 54000 loss: 292.6264875259399\n",
      "  batch 55000 loss: 365.668837348938\n",
      "  batch 56000 loss: 291.98230921936033\n",
      "  batch 57000 loss: 318.66838149261474\n",
      "  batch 58000 loss: 398.7269109764099\n",
      "  batch 59000 loss: 264.51663718795777\n",
      "  batch 60000 loss: 339.41542891311644\n",
      "  batch 61000 loss: 437.6862631149292\n",
      "  batch 62000 loss: 311.99026124572754\n",
      "  batch 63000 loss: 283.0006362953186\n",
      "  batch 64000 loss: 297.81918629455566\n",
      "  batch 65000 loss: 364.80148922348025\n",
      "  batch 66000 loss: 229.97300166702271\n",
      "  batch 67000 loss: 359.0935618972778\n",
      "  batch 68000 loss: 308.9061838912964\n",
      "  batch 69000 loss: 290.1114443283081\n",
      "  batch 70000 loss: 336.4696176147461\n",
      "  batch 71000 loss: 271.8322084083557\n",
      "  batch 72000 loss: 265.2663724937439\n",
      "  batch 73000 loss: 350.8938563652039\n",
      "  batch 74000 loss: 352.06841176223753\n",
      "  batch 75000 loss: 322.5486865005493\n",
      "  batch 76000 loss: 301.51782842636106\n",
      "  batch 77000 loss: 509.4283748397827\n",
      "  batch 78000 loss: 251.24374697113038\n",
      "  batch 79000 loss: 324.4256461105347\n",
      "  batch 80000 loss: 277.794009475708\n",
      "  batch 81000 loss: 378.6575372467041\n",
      "  batch 82000 loss: 466.1521392173767\n",
      "  batch 83000 loss: 409.85537274551393\n",
      "  batch 84000 loss: 271.2417983703613\n",
      "  batch 85000 loss: 303.46451466751097\n",
      "  batch 86000 loss: 415.98511278533937\n",
      "  batch 87000 loss: 273.15817379379274\n",
      "  batch 88000 loss: 399.8675464057922\n",
      "  batch 89000 loss: 404.8220112037659\n",
      "  batch 90000 loss: 358.2091588897705\n",
      "  batch 91000 loss: 357.64976165771486\n",
      "  batch 92000 loss: 405.6606539039612\n",
      "  batch 93000 loss: 350.00317876815797\n",
      "  batch 94000 loss: 325.2433792304993\n",
      "  batch 95000 loss: 301.81234469985964\n",
      "  batch 96000 loss: 322.9162746810913\n",
      "  batch 97000 loss: 390.63622759628294\n",
      "  batch 98000 loss: 364.0343134689331\n",
      "  batch 99000 loss: 371.8957875404358\n",
      "  batch 100000 loss: 398.8134397659302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "398.8134397659302"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6750c-45c9-4e5e-8f75-ca65bb85d670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e957309-c691-4f7f-9736-e345edb1a86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
